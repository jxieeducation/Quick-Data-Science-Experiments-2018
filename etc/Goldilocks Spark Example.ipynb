{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Spark - Goldilocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "import itertools\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with a pandas dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               panda_name  happiness  niceness  softness  sweetness\n",
      "0              Mama Panda       15.0      0.25    2467.0        0.0\n",
      "1              Papa Panda        2.0   1000.00      35.4        0.0\n",
      "2              Baby Panda       10.0      2.00      50.0        0.0\n",
      "3  Baby Panda's toy Panda        3.0      8.50       0.2       98.0\n"
     ]
    }
   ],
   "source": [
    "names = [\"Mama Panda\", \"Papa Panda\", \"Baby Panda\", \"Baby Panda's toy Panda\"]\n",
    "happiness = [15.0, 2.0, 10.0, 3.0]\n",
    "niceness = [0.25, 1000, 2.0, 8.5]\n",
    "softness = [2467.0, 35.4, 50.0, 0.2]\n",
    "sweetness = [0.0, 0.0, 0.0, 98.0]\n",
    "\n",
    "df = pd.DataFrame({\"happiness\": happiness, \"niceness\": niceness, \"softness\": softness, \"sweetness\": sweetness})\n",
    "df.insert(0, 'panda_name', names)\n",
    "\n",
    "print df.head()\n",
    "\n",
    "df = sqlCtx.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Problem\n",
    "* **input**: arbitrary list of integers n1...nk\n",
    "* **return**: nth best element in each column\n",
    "\n",
    "### example\n",
    "* input: [2, 4]\n",
    "* return:\n",
    "    * happiness: [3.0, 15.0]\n",
    "    * niceness: [2.0, 1000.0]\n",
    "    * softness: [35.4, 2467.0]\n",
    "    * sweetness: [0.0, 98.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Solution\n",
    "* loop through each column, mapping each row to a single value, then use Spark's sortBy with zipWithIndex function on each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [u\"Baby Panda's toy Panda\", u'Papa Panda'],\n",
       " 2: [3.0, 15.0],\n",
       " 3: [2.0, 1000.0],\n",
       " 4: [35.4, 2467.0],\n",
       " 5: [0.0, 98.0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findRankStatistics(df, ranks):\n",
    "    assert(all([rank > 0 for rank in ranks])) ## require all ranks to be > 0\n",
    "    \n",
    "    numCols = len(df.schema)\n",
    "    i = 0\n",
    "    results = {}\n",
    "    for i in range(numCols):\n",
    "        col = df.rdd.map(lambda row: row[i])\n",
    "        sortedCol = col.sortBy(lambda x: x).zipWithIndex()\n",
    "        ranksOnly = sortedCol.filter(\n",
    "            lambda (colValue, index): (index + 1) in ranks\n",
    "        ).keys()\n",
    "        rankedList = ranksOnly.collect()\n",
    "        results[i+1] = rankedList\n",
    "    return results\n",
    "\n",
    "findRankStatistics(df, [2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution works and is relatively robust, but it is very slow since it has to sort the\n",
    "data once for each column and does so iteratively. In other words, if we have 8,000\n",
    "columns we have to do 8,000 sorts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. groupByKey Solution\n",
    "* One simple solution to the Goldilocks problem is to use groupByKey to group the element in each column. GroupByKey returns an iterator of elements by each key\n",
    "* After converting the iterator to an array, we can sort the array and filter for the elements that correspond to our rank statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [u\"Baby Panda's toy Panda\", u'Papa Panda'],\n",
       " 1: [3.0, 15.0],\n",
       " 2: [2.0, 1000.0],\n",
       " 3: [35.4, 2467.0],\n",
       " 4: [0.0, 98.0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapToKeyValuePairs(df):\n",
    "    rowLength = len(df.schema)\n",
    "    return df.rdd.flatMap(lambda row: [(i, row[i]) for i in range(0, rowLength)])\n",
    "\n",
    "\n",
    "def findRankStatistics(df, ranks):\n",
    "    assert(all([rank > 0 for rank in ranks])) ## require all ranks to be > 0\n",
    "    \n",
    "    pairRDD = mapToKeyValuePairs(df)\n",
    "    groupColumns = pairRDD.groupByKey()\n",
    "    \n",
    "    def convertToArrayAndSort(iterable):\n",
    "        sortedIter = list(iterable)\n",
    "        sortedIter.sort()\n",
    "        return [val for i, val in enumerate(sortedIter) if (i+1) in ranks]\n",
    "        \n",
    "    ## mapValues doc: http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=mapvalues#pyspark.RDD.flatMapValues\n",
    "    return groupColumns.mapValues(convertToArrayAndSort).collectAsMap()\n",
    "\n",
    "findRankStatistics(df, [2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution has several advantages. \n",
    "* First, it gives the correct answer. \n",
    "* Second, it is very short and easy to understand. It leverages out-of-the-box Spark and Scala functions and so it introduces few edge cases and is relatively easy to test. On small data, particularly if the input data has many columns but few records, it is actually relatively efficient because it only requires one shuffle in the groupByKey step and because the sorting step can be computed as a narrow transformation on the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why GroupByKey fails\n",
    "* If you have read Learning Spark or spent much time working with Spark at scale, the results of the groupByKey approach to solving the Goldilocks problem shouldn’t surprise you as groupByKey is known to cause memory errors at scale. The reason is that the “groups” created by groupByKey are always iterators, which can’t be distributed.\n",
    "* This causes an expensive “shuffled read” step in which Spark has to read all of the shuffled data from disk and into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Secondary Sort Solution\n",
    "\n",
    "In order to mitigate the problem, instead of just sorting based on the column index, we can do a secondary sort. \n",
    "\n",
    "The function has four steps:\n",
    "1. Define a custom partitioner that partitions records according to the first element of the key.\n",
    "2. Define an implicit ordering on the values. This is only necessary because the function is generic. The implicit ordering on tuples is first value, second value. We just have to tell Spark to use that tuple ordering.\n",
    "3. Use repartitionAndSortWithinPartitions on the input RDD with the custom partitioner defined in step 1.\n",
    "4. Coalesce the items using a mapPartitions routine. We can leverage the fact that items with the same first key are on the same partition and that the elements within each partition are sorted first by the first ordering and then by the second ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## partitioner\n",
    "def ColumnIndexPartition(numPartitions):\n",
    "    assert(numPartitions >= 0)\n",
    "    \"\"\"Partition by the first item in the key tuple\"\"\"\n",
    "    def getPartition(x):\n",
    "        return abs(x[0]) % numPartitions\n",
    "    return getPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [u\"Baby Panda's toy Panda\", u'Papa Panda'],\n",
       " 1: [3.0, 15.0],\n",
       " 2: [2.0, 1000.0],\n",
       " 3: [35.4, 2467.0],\n",
       " 4: [0.0, 98.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findRankStatistics(df, targetRanks, partitions):\n",
    "    assert(all([rank > 0 for rank in targetRanks])) ## require all ranks to be > 0\n",
    "\n",
    "    pairRDD = mapToKeyValuePairs(df).map(lambda x: (x, 1))\n",
    "    partitioner = ColumnIndexPartition(partitions)\n",
    "    sorted = pairRDD.repartitionAndSortWithinPartitions(partitions, partitioner)\n",
    "    \n",
    "    def filterForTargetRanksFn(iterable):\n",
    "        currentColumnIndex = [-1] ## made these objects to be accessed in the filterFn without python scope problems\n",
    "        runningTotal = [0]\n",
    "        def filterFn(x):\n",
    "            ((colIndex, value), _) = x\n",
    "            if colIndex != currentColumnIndex[0]:\n",
    "                currentColumnIndex[0] = colIndex ## reset to the new column index\n",
    "                runningTotal[0] = 1\n",
    "            else:\n",
    "                runningTotal[0] += 1\n",
    "            return runningTotal[0] in targetRanks\n",
    "        return map(lambda x: x[0], filter(filterFn, iterable))\n",
    "    \n",
    "    filterForTargetIndex = sorted.mapPartitions(filterForTargetRanksFn, preservesPartitioning=True)\n",
    "    results = filterForTargetIndex.collect()\n",
    "    \n",
    "    resultsMap = {} ## just need to group them now\n",
    "    for i, val in results:\n",
    "        if i not in resultsMap: \n",
    "            resultsMap[i] = []\n",
    "        resultsMap[i].append(val)\n",
    "\n",
    "    return resultsMap\n",
    "\n",
    "\n",
    "findRankStatistics(df, [2, 4], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sort on Cell Values Solution \n",
    "\n",
    "Our solution is still not perfect.If the columns are relatively long, the repartitionAndSortWithinPartitions step may still lead to failures since it still requires one executor to be able to store all of the values associated with all of the columns that have the same hash value.\n",
    "\n",
    "1. Map the rows of data to pairs of (cell value, index).\n",
    "2. Perform a sortByKey operation on all tuples defined in step 1.\n",
    "3. Using mapPartitions, determine how many elements in each column are on\n",
    "each partition and collect that information to the driver.\n",
    "4. Perform a local computation on the result of step 3 to determine the location of\n",
    "each desired rank statistic. For example, suppose that we are looking for the 13th\n",
    "element. Suppose also that in step 3 we determined that the first partition had 10\n",
    "elements from column six. In this case, we can conclude that the 13th element\n",
    "will be the third largest element in column six on the second partition.\n",
    "5. Using the result of step 4, use another mapPartitions transformation to filter for\n",
    "the elements that correspond to the desired rank statistics. Collect this informa‐\n",
    "tion back to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getValueColumnPairs(dataFrame):\n",
    "    return dataFrame.rdd.flatMap(lambda row: [(val, index) for index, val in enumerate(row)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getColumnsFreqPerPartition(sortedValueColumnPairs, numOfColumns):\n",
    "    zero = [0 for i in range(numOfColumns)]\n",
    "    def aggregateColumnFrequencies(partitionIndex, valueColumnPairs):\n",
    "        for val, index in valueColumnPairs:\n",
    "            zero[index] += 1\n",
    "        return [(partitionIndex, list(zero))]\n",
    "    return sortedValueColumnPairs.mapPartitionsWithIndex(aggregateColumnFrequencies).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRanksLocationsWithinEachPart(targetRanks, partitionColumnsFreq, numOfColumns):\n",
    "    runningTotal = [0 for i in range(numOfColumns)]\n",
    "    \n",
    "    def partitionColumnsFreqFn((partitionIndex, columnsFreq)):\n",
    "        relevantIndexList = []\n",
    "        for colIndex, colCount in enumerate(columnsFreq):\n",
    "            runningTotalCol = runningTotal[colIndex]\n",
    "            ranksHere = filter(lambda rank: runningTotalCol < rank and (runningTotalCol + colCount) >= rank, \n",
    "                               targetRanks)\n",
    "            relevantIndexList += map(lambda rank: [colIndex, rank - runningTotalCol], ranksHere)\n",
    "            runningTotal[colIndex] += colCount\n",
    "        return [partitionIndex, filter(lambda x: x != [], relevantIndexList)]\n",
    "    \n",
    "    return map(partitionColumnsFreqFn, sorted(partitionColumnsFreq, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTargetRanksIteratively(sortedValueColumnPairs, ranksLocations):\n",
    "    def sortedValueColumnPairsFn(partitionIndex, valueColumnPairs):\n",
    "        targetsInThisPart = ranksLocations[partitionIndex][1]\n",
    "        if targetsInThisPart:\n",
    "            columnsRelativeIndex = {}\n",
    "            for k, g in itertools.groupby(sorted(targetsInThisPart), key=lambda x: x[0]):\n",
    "                columnsRelativeIndex[k] = [pair[1] for pair in g]\n",
    "            columnsInThisPart = list(set(map(lambda x: x[0], targetsInThisPart)))\n",
    "            runningTotals = {}\n",
    "            for i in columnsInThisPart:\n",
    "                runningTotals[i] = 0\n",
    "            \n",
    "            def valueColumnPairsFn((value, colIndex)):\n",
    "                if colIndex not in runningTotals:\n",
    "                    return False\n",
    "                total = runningTotals[colIndex] + 1\n",
    "                runningTotals[colIndex] = total\n",
    "                thisPairIsTheRankStatistic = total in columnsRelativeIndex[colIndex]\n",
    "                return thisPairIsTheRankStatistic\n",
    "            \n",
    "            return map(lambda (x,y): (y,x), filter(valueColumnPairsFn, valueColumnPairs))         \n",
    "        else:\n",
    "            return iter([])\n",
    "        return targetsInThisPart\n",
    "             \n",
    "    return sortedValueColumnPairs.mapPartitionsWithIndex(sortedValueColumnPairsFn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [u\"Baby Panda's toy Panda\", u'Papa Panda'],\n",
       " 1: [3.0, 15.0],\n",
       " 2: [2.0, 1000.0],\n",
       " 3: [35.4, 2467.0],\n",
       " 4: [0.0, 98.0]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findRankStatistics(dataFrame, targetRanks):\n",
    "    valueColumnPairs = getValueColumnPairs(dataFrame)\n",
    "    sortedValueColumnPairs = valueColumnPairs.sortByKey()\n",
    "    sortedValueColumnPairs.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    numOfColumns = len(dataFrame.schema)\n",
    "    partitionColumnsFreq = getColumnsFreqPerPartition(sortedValueColumnPairs, numOfColumns)\n",
    "    ranksLocations = getRanksLocationsWithinEachPart(targetRanks, partitionColumnsFreq, numOfColumns)\n",
    "    targetRanksValues = findTargetRanksIteratively(sortedValueColumnPairs, ranksLocations)\n",
    "\n",
    "    results = targetRanksValues.collect()\n",
    "    resultsMap = {} ## just need to group them now\n",
    "    for i, val in results:\n",
    "        if i not in resultsMap: \n",
    "            resultsMap[i] = []\n",
    "        resultsMap[i].append(val)\n",
    "    return resultsMap\n",
    "    \n",
    "findRankStatistics(df, [2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Goldilocks Postmortem\n",
    "\n",
    "* Goldilocks Version 1: Iterative Solution. Our first solution iteratively looped through each group and performed a distributed sort, resulting in one stage and one expensive distributed sort per group.\n",
    "\n",
    "* Goldilocks Version 2: groupByKey Solution. The next solution used groupByKey shuffle records associated with the same group onto the same partition. Then we were able to sort each group in one stage by using mapPartitions to sort the values in each group.\n",
    "\n",
    "* Goldilocks Version 3: Secondary Sort. Using the secondary sort technique, we improved our groupByKey solution by replacing the groupByKey and sorting steps with the repartitionAndSortWithinPartitions function to push the work of sorting each group into the shuffle stage.\n",
    "\n",
    "* Goldilocks Version 4: Sort on Cell Values. Next, we realized that it was possible to solve the problem using only one sort on the value of each record, rather than the group. We developed a solution that keyed the records by value (rather than by group/column index), sorted all the records, and then performed a series of narrow transformations to collect the results. We expected the new sorting keys (the values in the columns) to contain fewer duplicates than the the size of each group, which we used as a key in version 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
