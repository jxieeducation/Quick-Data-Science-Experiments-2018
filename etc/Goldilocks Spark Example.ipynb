{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Spark - Goldilocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "import itertools\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with a pandas dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               panda_name  happiness  niceness  softness  sweetness\n",
      "0              Mama Panda       15.0      0.25    2467.0        0.0\n",
      "1              Papa Panda        2.0   1000.00      35.4        0.0\n",
      "2              Baby Panda       10.0      2.00      50.0        0.0\n",
      "3  Baby Panda's toy Panda        3.0      8.50       0.2       98.0\n"
     ]
    }
   ],
   "source": [
    "names = [\"Mama Panda\", \"Papa Panda\", \"Baby Panda\", \"Baby Panda's toy Panda\"]\n",
    "happiness = [15.0, 2.0, 10.0, 3.0]\n",
    "niceness = [0.25, 1000, 2.0, 8.5]\n",
    "softness = [2467.0, 35.4, 50.0, 0.2]\n",
    "sweetness = [0.0, 0.0, 0.0, 98.0]\n",
    "\n",
    "df = pd.DataFrame({\"happiness\": happiness, \"niceness\": niceness, \"softness\": softness, \"sweetness\": sweetness})\n",
    "df.insert(0, 'panda_name', names)\n",
    "\n",
    "print df.head()\n",
    "\n",
    "df = sqlCtx.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goldilocks Problem\n",
    "* **input**: arbitrary list of integers n1...nk\n",
    "* **return**: nth best element in each column\n",
    "\n",
    "### example\n",
    "* input: [2, 4]\n",
    "* return:\n",
    "    * happiness: [3.0, 15.0]\n",
    "    * niceness: [2.0, 1000.0]\n",
    "    * softness: [35.4, 2467.0]\n",
    "    * sweetness: [0.0, 98.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Solution\n",
    "* loop through each column, mapping each row to a single value, then use Spark's sortBy with zipWithIndex function on each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [u\"Baby Panda's toy Panda\", u'Papa Panda'],\n",
       " 2: [3.0, 15.0],\n",
       " 3: [2.0, 1000.0],\n",
       " 4: [35.4, 2467.0],\n",
       " 5: [0.0, 98.0]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findRankStatistics(df, ranks):\n",
    "    assert(all([rank > 0 for rank in ranks])) ## require all ranks to be > 0\n",
    "    \n",
    "    numCols = len(df.schema)\n",
    "    i = 0\n",
    "    results = {}\n",
    "    for i in range(numCols):\n",
    "        col = df.rdd.map(lambda row: row[i])\n",
    "        sortedCol = col.sortBy(lambda x: x).zipWithIndex()\n",
    "        ranksOnly = sortedCol.filter(\n",
    "            lambda (colValue, index): (index + 1) in ranks\n",
    "        ).keys()\n",
    "        rankedList = ranksOnly.collect()\n",
    "        results[i+1] = rankedList\n",
    "    return results\n",
    "\n",
    "findRankStatistics(df, [2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution works and is relatively robust, but it is very slow since it has to sort the\n",
    "data once for each column and does so iteratively. In other words, if we have 8,000\n",
    "columns we have to do 8,000 sorts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. groupByKey Solution\n",
    "* One simple solution to the Goldilocks problem is to use groupByKey to group the element in each column. GroupByKey returns an iterator of elements by each key\n",
    "* After converting the iterator to an array, we can sort the array and filter for the elements that correspond to our rank statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [u\"Baby Panda's toy Panda\", u'Papa Panda'],\n",
       " 1: [3.0, 15.0],\n",
       " 2: [2.0, 1000.0],\n",
       " 3: [35.4, 2467.0],\n",
       " 4: [0.0, 98.0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapToKeyValuePairs(df):\n",
    "    rowLength = len(df.schema)\n",
    "    return df.rdd.flatMap(lambda row: [(i, row[i]) for i in range(0, rowLength)])\n",
    "\n",
    "\n",
    "def findRankStatistics(df, ranks):\n",
    "    assert(all([rank > 0 for rank in ranks])) ## require all ranks to be > 0\n",
    "    \n",
    "    pairRDD = mapToKeyValuePairs(df)\n",
    "    groupColumns = pairRDD.groupByKey()\n",
    "    \n",
    "    def convertToArrayAndSort(iterable):\n",
    "        sortedIter = list(iterable)\n",
    "        sortedIter.sort()\n",
    "        return [val for i, val in enumerate(sortedIter) if (i+1) in ranks]\n",
    "        \n",
    "    ## mapValues doc: http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=mapvalues#pyspark.RDD.flatMapValues\n",
    "    return groupColumns.mapValues(convertToArrayAndSort).collectAsMap()\n",
    "\n",
    "findRankStatistics(df, [2, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution has several advantages. \n",
    "* First, it gives the correct answer. \n",
    "* Second, it is very short and easy to understand. It leverages out-of-the-box Spark and Scala functions and so it introduces few edge cases and is relatively easy to test. On small data, particularly if the input data has many columns but few records, it is actually relatively efficient because it only requires one shuffle in the groupByKey step and because the sorting step can be computed as a narrow transformation on the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why GroupByKey fails\n",
    "* If you have read Learning Spark or spent much time working with Spark at scale, the results of the groupByKey approach to solving the Goldilocks problem shouldn’t surprise you as groupByKey is known to cause memory errors at scale. The reason is that the “groups” created by groupByKey are always iterators, which can’t be distributed.\n",
    "* This causes an expensive “shuffled read” step in which Spark has to read all of the shuffled data from disk and into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Secondary Sort Solution\n",
    "\n",
    "In order to mitigate the problem, instead of just sorting based on the column index, we can do a secondary sort. \n",
    "\n",
    "The function has four steps:\n",
    "1. Define a custom partitioner that partitions records according to the first element of the key.\n",
    "2. Define an implicit ordering on the values. This is only necessary because the function is generic. The implicit ordering on tuples is first value, second value. We just have to tell Spark to use that tuple ordering.\n",
    "3. Use repartitionAndSortWithinPartitions on the input RDD with the custom partitioner defined in step 1.\n",
    "4. Coalesce the items using a mapPartitions routine. We can leverage the fact that items with the same first key are on the same partition and that the elements within each partition are sorted first by the first ordering and then by the second ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## partitioner\n",
    "def ColumnIndexPartition(numPartitions):\n",
    "    assert(numPartitions >= 0)\n",
    "    \"\"\"Partition by the first item in the key tuple\"\"\"\n",
    "    def getPartition(x):\n",
    "        return abs(x[0]) % numPartitions\n",
    "    return getPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [u\"Baby Panda's toy Panda\", u'Papa Panda'],\n",
       " 1: [3.0, 15.0],\n",
       " 2: [2.0, 1000.0],\n",
       " 3: [35.4, 2467.0],\n",
       " 4: [0.0, 98.0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def findRankStatistics(df, targetRanks, partitions):\n",
    "    assert(all([rank > 0 for rank in targetRanks])) ## require all ranks to be > 0\n",
    "\n",
    "    pairRDD = mapToKeyValuePairs(df).map(lambda x: (x, 1))\n",
    "    partitioner = ColumnIndexPartition(partitions)\n",
    "    sorted = pairRDD.repartitionAndSortWithinPartitions(partitions, partitioner)\n",
    "    \n",
    "    def filterForTargetRanksFn(iterable):\n",
    "        currentColumnIndex = [-1] ## made these objects to be accessed in the filterFn without python scope problems\n",
    "        runningTotal = [0]\n",
    "        def filterFn(x):\n",
    "            ((colIndex, value), _) = x\n",
    "            if colIndex != currentColumnIndex[0]:\n",
    "                currentColumnIndex[0] = colIndex ## reset to the new column index\n",
    "                runningTotal[0] = 1\n",
    "            else:\n",
    "                runningTotal[0] += 1\n",
    "            return runningTotal[0] in targetRanks\n",
    "        return map(lambda x: x[0], filter(filterFn, iterable))\n",
    "    \n",
    "    filterForTargetIndex = sorted.mapPartitions(filterForTargetRanksFn, preservesPartitioning=True)\n",
    "    results = filterForTargetIndex.collect()\n",
    "    \n",
    "    resultsMap = {} ## just need to group them now\n",
    "    for i, val in results:\n",
    "        if i not in resultsMap: \n",
    "            resultsMap[i] = []\n",
    "        resultsMap[i].append(val)\n",
    "\n",
    "    return resultsMap\n",
    "\n",
    "\n",
    "findRankStatistics(df, [2, 4], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solution \n",
    "\n",
    "Our solution is still not perfect.If the columns are relatively long, the repartitionAndSortWithinPartitions step may still lead to failures since it still requires one executor to be able to store all of the values associated with all of the columns that have the same hash value.\n",
    "\n",
    "1. Map the rows of data to pairs of (cell value, index).\n",
    "2. Perform a sortByKey operation on all tuples defined in step 1.\n",
    "3. Using mapPartitions, determine how many elements in each column are on\n",
    "each partition and collect that information to the driver.\n",
    "4. Perform a local computation on the result of step 3 to determine the location of\n",
    "each desired rank statistic. For example, suppose that we are looking for the 13th\n",
    "element. Suppose also that in step 3 we determined that the first partition had 10\n",
    "elements from column six. In this case, we can conclude that the 13th element\n",
    "will be the third largest element in column six on the second partition.\n",
    "5. Using the result of step 4, use another mapPartitions transformation to filter for\n",
    "the elements that correspond to the desired rank statistics. Collect this informa‐\n",
    "tion back to the driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getValueColumnPairs(dataFrame):\n",
    "    return dataFrame.rdd.flatMap(lambda row: [(val, index) for index, val in enumerate(row)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getColumnsFreqPerPartition(sortedValueColumnPairs, numOfColumns):\n",
    "    zero = [0 for i in range(numOfColumns)]\n",
    "    def aggregateColumnFrequencies(partitionIndex, valueColumnPairs):\n",
    "        for val, index in valueColumnPairs:\n",
    "            zero[index-1] += 1\n",
    "        return [(partitionIndex, list(zero))]\n",
    "    return sortedValueColumnPairs.mapPartitionsWithIndex(aggregateColumnFrequencies).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRanksLocationsWithinEachPart(targetRanks, partitionColumnsFreq, numOfColumns):\n",
    "    runningTotal = [0 for i in range(numOfColumns)]\n",
    "    \n",
    "    def partitionColumnsFreqFn((partitionIndex, columnsFreq)):\n",
    "        relevantIndexList = []\n",
    "        for colIndex, colCount in enumerate(columnsFreq):\n",
    "            runningTotalCol = runningTotal[colIndex]\n",
    "            ranksHere = filter(lambda rank: runningTotalCol < rank and (runningTotalCol + colCount) >= rank, \n",
    "                               targetRanks)\n",
    "            relevantIndexList += map(lambda rank: (colIndex, rank - runningTotalCol), ranksHere)\n",
    "            runningTotal[colIndex] += colCount\n",
    "        return [partitionIndex, filter(lambda x: x != [], relevantIndexList)]\n",
    "    \n",
    "    return map(partitionColumnsFreqFn, sorted(partitionColumnsFreq, key=lambda x: x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTargetRanksIteratively(sortedValueColumnPairs, ranksLocations):\n",
    "    def sortedValueColumnPairsFn(partitionIndex, valueColumnPairs):\n",
    "        return [map(lambda x: x[1], ranksLocations[partitionIndex])]\n",
    "        targetsInThisPart = [(x[1] for x in ranksLocations[partitionIndex])]\n",
    "        if targetsInThisPart:\n",
    "            columnsRelativeIndex = [(k, list(g)[1]) for k, g in itertools.groupby(sorted(targetsInThisPart), key=lambda x: x[0])]\n",
    "            columnsInThisPart = list(set(map(lambda x: x[0], targetsInThisPart)))\n",
    "#             return columnsInThisPart\n",
    "            runningTotals = {}\n",
    "            for i in columnsInThisPart:\n",
    "                runningTotals[i] = (i, 0)\n",
    "            def valueColumnPairsFn((value, colIndex)):\n",
    "                total = runningTotals[colIndex] + 1\n",
    "                runningTotals[colIndex] = total\n",
    "                thisPairIsTheRankStatistic = total in columnsRelativeIndex[colIndex]      \n",
    "            return map(lambda (x,y): (y,x), filter(valueColumnPairsFn, valueColumnPairs))         \n",
    "        else:\n",
    "            return iter([])\n",
    "        return targetsInThisPart\n",
    "             \n",
    "    return sortedValueColumnPairs.mapPartitionsWithIndex(sortedValueColumnPairsFn)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 998.0 failed 1 times, most recent failure: Lost task 5.0 in stage 998.0 (TID 6420, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in sortedValueColumnPairsFn\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in <lambda>\nTypeError: 'int' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in sortedValueColumnPairsFn\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in <lambda>\nTypeError: 'int' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-311-3fa63850dbd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtargetRanksValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mfindRankStatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-311-3fa63850dbd4>\u001b[0m in \u001b[0;36mfindRankStatistics\u001b[0;34m(dataFrame, targetRanks)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtargetRanksValues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindTargetRanksIteratively\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msortedValueColumnPairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranksLocations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mtargetRanksValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtargetRanksValues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAsMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 998.0 failed 1 times, most recent failure: Lost task 5.0 in stage 998.0 (TID 6420, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in sortedValueColumnPairsFn\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in <lambda>\nTypeError: 'int' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/Cellar/apache-spark/2.2.0/libexec/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in sortedValueColumnPairsFn\n  File \"<ipython-input-310-b964ff45c2ce>\", line 3, in <lambda>\nTypeError: 'int' object has no attribute '__getitem__'\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "def findRankStatistics(dataFrame, targetRanks):\n",
    "    valueColumnPairs = getValueColumnPairs(dataFrame)\n",
    "    sortedValueColumnPairs = valueColumnPairs.sortByKey()\n",
    "    sortedValueColumnPairs.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
    "    numOfColumns = len(dataFrame.schema)\n",
    "    partitionColumnsFreq = getColumnsFreqPerPartition(sortedValueColumnPairs, numOfColumns)\n",
    "    ranksLocations = getRanksLocationsWithinEachPart(targetRanks, partitionColumnsFreq, numOfColumns)\n",
    "#     print ranksLocations\n",
    "    targetRanksValues = findTargetRanksIteratively(sortedValueColumnPairs, ranksLocations)\n",
    "\n",
    "    print targetRanksValues.collect()\n",
    "    return targetRanksValues.groupByKey().collectAsMap()\n",
    "    \n",
    "findRankStatistics(df, [2, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
